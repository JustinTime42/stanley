# PRP-10: Browser Automation Integration

## Goal

**Feature Goal**: Implement comprehensive browser automation and E2E testing capabilities with Playwright integration, enabling automated UI testing, user journey validation, visual regression testing, and cross-browser compatibility verification for web applications generated or tested by the agent swarm.

**Deliverable**: Browser automation service with Playwright MCP integration, page object model generation, user journey test creation, visual regression testing, cross-browser execution capabilities, and integration with the existing Tester agent and test generation infrastructure.

**Success Definition**:

- Support for 3+ browsers (Chromium, Firefox, WebKit/Safari)
- E2E test generation in <10 seconds for standard user journeys
- Visual regression testing with 95%+ accuracy in change detection
- Page Object Model (POM) auto-generation from DOM analysis
- User journey recording and playback capabilities
- Network interception and API mocking support
- Parallel test execution reducing test time by 60%
- Accessibility testing achieving WCAG 2.1 AA compliance checks
- Performance metrics collection (Core Web Vitals)

## Why

- Current testing infrastructure (PRP-09) lacks UI and E2E testing capabilities
- No automated way to validate user interfaces generated by agents
- Missing visual regression testing for UI changes
- No cross-browser compatibility verification
- Cannot validate full user journeys and workflows
- No accessibility or performance testing automation
- Critical for validating complete applications, not just code units
- Essential for ensuring quality of web applications built by the agent swarm

## What

Implement a sophisticated browser automation system using Playwright that provides comprehensive E2E testing capabilities, including page object model generation, user journey recording/playback, visual regression testing, cross-browser validation, accessibility checks, and performance monitoring, all integrated with the existing test generation and validation infrastructure.

### Success Criteria

- [ ] Playwright MCP integration operational with 3+ browsers
- [ ] Page Object Model generation from DOM analysis
- [ ] User journey recording and test generation
- [ ] Visual regression testing with screenshot comparison
- [ ] Cross-browser test execution in parallel
- [ ] Accessibility testing with WCAG compliance reports
- [ ] Performance metrics collection and validation
- [ ] Network stubbing and API mocking capabilities

## All Needed Context

### Context Completeness Check

_If someone knew nothing about this codebase, would they have everything needed to implement this successfully?_ YES - This PRP includes complete Playwright integration patterns, POM generation strategies, visual testing approaches, accessibility testing methods, and integration with existing test infrastructure.

### Documentation & References

```yaml
- url: https://playwright.dev/docs/intro
  why: Playwright documentation for browser automation
  critical: Core API for browser control, element selection, and test execution
  section: Getting started and API reference

- url: https://playwright.dev/docs/test-snapshots
  why: Visual regression testing with Playwright
  critical: Screenshot comparison and visual diff algorithms
  section: Visual comparisons

- url: https://playwright.dev/docs/accessibility-testing
  why: Accessibility testing capabilities
  critical: WCAG compliance checking and aria attributes validation

- url: https://github.com/microsoft/playwright-test
  why: Playwright Test runner for E2E testing
  critical: Test organization, fixtures, and parallel execution

- file: src/agents/tester.py
  why: Existing Tester agent that will use browser automation
  pattern: BaseAgent inheritance, test execution integration
  gotcha: Already handles unit/integration tests from PRP-09

- file: src/testing/test_generator.py
  why: Test generation infrastructure from PRP-09
  pattern: TestGenerator orchestration, test suite creation
  gotcha: Need to extend for E2E test generation

- file: src/tools/implementations/test_tools.py
  why: Test execution tools that will include Playwright
  pattern: Tool implementation pattern, async execution
  gotcha: Must handle browser lifecycle management

- file: src/analysis/ast_parser.py
  why: Code analysis for understanding component structure
  pattern: AST parsing for React/Vue/Angular components
  gotcha: Use for component prop extraction

- url: https://github.com/modelcontextprotocol/servers/tree/main/playwright
  why: Playwright MCP server implementation reference
  critical: MCP protocol integration for browser control

- url: https://pptr.dev/guides/page-objects
  why: Page Object Model design patterns
  critical: Best practices for maintainable test structure

- url: https://github.com/applitools/eyes-playwright
  why: Visual testing integration options
  critical: Advanced visual regression capabilities
```

### Current Codebase Tree

```bash
agent-swarm/
├── src/
│   ├── agents/
│   │   ├── tester.py               # Test execution agent
│   │   ├── validator.py            # Validation agent
│   │   └── debugger.py             # Debugging agent
│   ├── testing/                    # From PRP-09
│   │   ├── test_generator.py       # Test generation orchestrator
│   │   ├── frameworks/             # Framework-specific generators
│   │   ├── data_generator.py       # Test data generation
│   │   └── coverage_analyzer.py    # Coverage analysis
│   ├── tools/
│   │   └── implementations/
│   │       ├── test_tools.py       # Test execution tools
│   │       └── docker_tools.py     # Container operations
│   ├── services/
│   │   ├── testing_service.py      # Testing orchestration
│   │   └── tool_service.py         # Tool management
│   └── models/
│       ├── testing_models.py       # Test-related models
│       └── tool_models.py          # Tool execution models
```

### Desired Codebase Tree with Files to be Added

```bash
agent-swarm/
├── src/
│   ├── browser/                              # NEW: Browser automation subsystem
│   │   ├── __init__.py                       # Export main interfaces
│   │   ├── base.py                           # BaseBrowserAutomation abstract class
│   │   ├── playwright_integration.py         # Playwright MCP integration
│   │   ├── browser_manager.py                # Browser lifecycle management
│   │   ├── page_analyzer.py                  # DOM analysis and POM generation
│   │   ├── journey_recorder.py               # User journey recording
│   │   ├── visual_tester.py                  # Visual regression testing
│   │   ├── accessibility_tester.py           # Accessibility validation
│   │   ├── performance_monitor.py            # Performance metrics collection
│   │   ├── network_interceptor.py            # Network stubbing and mocking
│   │   ├── pom/                              # Page Object Model
│   │   │   ├── __init__.py
│   │   │   ├── pom_generator.py              # POM auto-generation
│   │   │   ├── element_selector.py           # Smart element selection
│   │   │   └── templates/                    # POM templates
│   │   │       ├── react_pom.py              # React component POM
│   │   │       ├── vue_pom.py                # Vue component POM
│   │   │       └── angular_pom.py            # Angular component POM
│   │   ├── strategies/                       # Test strategies
│   │   │   ├── __init__.py
│   │   │   ├── user_flow_strategy.py         # User flow testing
│   │   │   ├── monkey_testing.py             # Random interaction testing
│   │   │   └── smoke_testing.py              # Critical path testing
│   │   └── reporters/                        # Test reporting
│   │       ├── __init__.py
│   │       ├── html_reporter.py              # HTML test reports
│   │       ├── accessibility_reporter.py     # WCAG compliance reports
│   │       └── performance_reporter.py       # Performance reports
│   ├── models/
│   │   └── browser_models.py                 # NEW: Browser automation models
│   ├── services/
│   │   └── browser_service.py                # NEW: Browser automation service
│   ├── tools/implementations/
│   │   └── browser_tools.py                  # NEW: Browser automation tools
│   ├── agents/
│   │   └── tester.py                         # MODIFY: Add E2E testing capabilities
│   └── tests/
│       └── browser/                          # NEW: Browser automation tests
│           ├── test_playwright_integration.py
│           ├── test_pom_generation.py
│           ├── test_visual_regression.py
│           └── test_accessibility.py
```

### Known Gotchas of our Codebase & Library Quirks

```python
# CRITICAL: Browser contexts must be properly managed to avoid resource leaks
# Always use context managers or explicit cleanup

# CRITICAL: Playwright requires browser binaries to be installed
# Run: playwright install chromium firefox webkit

# CRITICAL: Visual regression baselines must be OS/browser specific
# Different rendering on different platforms

# CRITICAL: Network interception can affect test timing
# Account for network delays in assertions

# CRITICAL: Parallel execution requires isolated browser contexts
# Each test must have its own context to avoid interference

# CRITICAL: Accessibility testing has false positives
# Manual review needed for certain WCAG criteria

# CRITICAL: Page Object Models must handle dynamic content
# Use proper wait strategies and element state checks

# CRITICAL: Screenshot comparison needs tolerance settings
# Pixel-perfect matching fails due to anti-aliasing

# CRITICAL: Performance metrics vary by hardware
# Establish baselines per environment
```

## Implementation Blueprint

### Data Models and Structure

```python
# src/models/browser_models.py
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Optional, Literal, Set
from enum import Enum
from datetime import datetime

class BrowserType(str, Enum):
    """Supported browser engines."""
    CHROMIUM = "chromium"
    FIREFOX = "firefox"
    WEBKIT = "webkit"
    CHROME = "chrome"
    EDGE = "edge"
    SAFARI = "safari"

class DeviceType(str, Enum):
    """Device emulation types."""
    DESKTOP = "desktop"
    MOBILE = "mobile"
    TABLET = "tablet"

class TestType(str, Enum):
    """Browser test types."""
    E2E = "e2e"
    VISUAL = "visual"
    ACCESSIBILITY = "accessibility"
    PERFORMANCE = "performance"
    SMOKE = "smoke"
    REGRESSION = "regression"

class Viewport(BaseModel):
    """Browser viewport configuration."""
    width: int = Field(default=1280, description="Viewport width")
    height: int = Field(default=720, description="Viewport height")
    device_scale_factor: float = Field(default=1.0, description="Device pixel ratio")
    is_mobile: bool = Field(default=False, description="Mobile viewport")
    has_touch: bool = Field(default=False, description="Touch support")

class PageElement(BaseModel):
    """Represents a page element for POM."""
    name: str = Field(description="Element name")
    selector: str = Field(description="CSS/XPath selector")
    element_type: str = Field(description="Element type (button, input, etc.)")
    attributes: Dict[str, str] = Field(default_factory=dict, description="Element attributes")
    text_content: Optional[str] = Field(default=None, description="Element text")
    is_visible: bool = Field(default=True, description="Visibility state")
    is_enabled: bool = Field(default=True, description="Enabled state")
    aria_label: Optional[str] = Field(default=None, description="Accessibility label")
    data_testid: Optional[str] = Field(default=None, description="Test ID attribute")

class PageObjectModel(BaseModel):
    """Page Object Model representation."""
    id: str = Field(description="POM identifier")
    url: str = Field(description="Page URL")
    name: str = Field(description="Page name")
    elements: Dict[str, PageElement] = Field(default_factory=dict, description="Page elements")

    # Actions
    actions: List[str] = Field(default_factory=list, description="Available actions")

    # Locators
    locator_strategy: str = Field(default="css", description="Primary locator strategy")
    custom_locators: Dict[str, str] = Field(default_factory=dict, description="Custom locators")

    # Component structure (for React/Vue/Angular)
    components: List[Dict[str, Any]] = Field(default_factory=list, description="Component tree")

    # Metadata
    framework: Optional[str] = Field(default=None, description="Frontend framework")
    generated_at: datetime = Field(default_factory=datetime.now)
    last_updated: datetime = Field(default_factory=datetime.now)

class UserJourney(BaseModel):
    """User journey test specification."""
    id: str = Field(description="Journey identifier")
    name: str = Field(description="Journey name")
    description: str = Field(description="Journey description")

    # Steps
    steps: List["JourneyStep"] = Field(default_factory=list, description="Journey steps")

    # Assertions
    assertions: List[str] = Field(default_factory=list, description="Journey assertions")
    expected_outcomes: Dict[str, Any] = Field(default_factory=dict, description="Expected results")

    # Configuration
    start_url: str = Field(description="Starting URL")
    viewport: Viewport = Field(default_factory=Viewport, description="Viewport settings")
    browser: BrowserType = Field(default=BrowserType.CHROMIUM, description="Browser to use")

    # Timing
    timeout_ms: int = Field(default=30000, description="Journey timeout")
    step_delay_ms: int = Field(default=0, description="Delay between steps")

    # Recording metadata
    recorded_at: Optional[datetime] = Field(default=None, description="Recording timestamp")
    duration_ms: Optional[int] = Field(default=None, description="Journey duration")

class JourneyStep(BaseModel):
    """Individual step in a user journey."""
    step_number: int = Field(description="Step sequence number")
    action: str = Field(description="Action type (click, fill, navigate, etc.)")
    target: Optional[str] = Field(default=None, description="Target selector")
    value: Optional[Any] = Field(default=None, description="Input value if applicable")

    # Wait conditions
    wait_for: Optional[str] = Field(default=None, description="Wait condition")
    wait_timeout_ms: int = Field(default=5000, description="Wait timeout")

    # Validation
    validate: Optional[str] = Field(default=None, description="Validation expression")
    screenshot: bool = Field(default=False, description="Take screenshot after step")

    # Network
    wait_for_network: bool = Field(default=False, description="Wait for network idle")
    expected_requests: List[str] = Field(default_factory=list, description="Expected API calls")

class VisualTestResult(BaseModel):
    """Visual regression test result."""
    test_id: str = Field(description="Test identifier")
    page_url: str = Field(description="Page URL tested")

    # Screenshots
    baseline_path: str = Field(description="Baseline screenshot path")
    actual_path: str = Field(description="Actual screenshot path")
    diff_path: Optional[str] = Field(default=None, description="Diff image path")

    # Comparison results
    match_percentage: float = Field(description="Similarity percentage")
    pixel_difference: int = Field(description="Number of different pixels")
    is_match: bool = Field(description="Whether images match within threshold")

    # Diff regions
    diff_regions: List[Dict[str, int]] = Field(
        default_factory=list,
        description="Regions with differences (x, y, width, height)"
    )

    # Configuration
    threshold: float = Field(default=0.01, description="Difference threshold")
    ignore_regions: List[Dict[str, int]] = Field(
        default_factory=list,
        description="Ignored regions"
    )

    # Metadata
    browser: BrowserType = Field(description="Browser used")
    viewport: Viewport = Field(description="Viewport settings")
    timestamp: datetime = Field(default_factory=datetime.now)

class AccessibilityIssue(BaseModel):
    """Accessibility violation details."""
    id: str = Field(description="Issue identifier")
    impact: Literal["minor", "moderate", "serious", "critical"] = Field(description="Impact level")
    rule_id: str = Field(description="WCAG rule ID")
    description: str = Field(description="Issue description")
    help_text: str = Field(description="How to fix")

    # Location
    selector: str = Field(description="Element selector")
    html: str = Field(description="Element HTML")

    # WCAG info
    wcag_criteria: List[str] = Field(default_factory=list, description="WCAG criteria")
    wcag_level: Literal["A", "AA", "AAA"] = Field(description="WCAG level")

    # Fix suggestion
    fix_suggestion: Optional[str] = Field(default=None, description="Suggested fix")

class PerformanceMetrics(BaseModel):
    """Performance metrics from browser."""
    url: str = Field(description="Page URL")

    # Core Web Vitals
    lcp: float = Field(description="Largest Contentful Paint (ms)")
    fid: float = Field(description="First Input Delay (ms)")
    cls: float = Field(description="Cumulative Layout Shift")

    # Additional metrics
    ttfb: float = Field(description="Time to First Byte (ms)")
    fcp: float = Field(description="First Contentful Paint (ms)")
    tti: float = Field(description="Time to Interactive (ms)")
    speed_index: float = Field(description="Speed Index")

    # Resource timing
    dom_content_loaded: float = Field(description="DOM Content Loaded (ms)")
    load_complete: float = Field(description="Load Complete (ms)")

    # Network
    total_requests: int = Field(description="Total network requests")
    total_size_kb: float = Field(description="Total transfer size (KB)")

    # Memory
    js_heap_size_mb: float = Field(description="JS heap size (MB)")

    # Thresholds
    passes_cwv: bool = Field(description="Passes Core Web Vitals")

    # Metadata
    browser: BrowserType = Field(description="Browser used")
    timestamp: datetime = Field(default_factory=datetime.now)

class NetworkMock(BaseModel):
    """Network request mock configuration."""
    url_pattern: str = Field(description="URL pattern to match")
    method: str = Field(default="GET", description="HTTP method")

    # Response
    status: int = Field(default=200, description="Response status code")
    headers: Dict[str, str] = Field(default_factory=dict, description="Response headers")
    body: Optional[Any] = Field(default=None, description="Response body")

    # Behavior
    delay_ms: int = Field(default=0, description="Response delay")
    abort: bool = Field(default=False, description="Abort request")

    # Conditions
    times: Optional[int] = Field(default=None, description="Number of times to mock")
    predicate: Optional[str] = Field(default=None, description="Condition function")

class BrowserTestSuite(BaseModel):
    """Browser test suite specification."""
    id: str = Field(description="Suite identifier")
    name: str = Field(description="Suite name")

    # Tests
    journeys: List[UserJourney] = Field(default_factory=list, description="User journeys")
    visual_tests: List[Dict[str, Any]] = Field(default_factory=list, description="Visual tests")
    accessibility_tests: List[Dict[str, Any]] = Field(default_factory=list, description="A11y tests")
    performance_tests: List[Dict[str, Any]] = Field(default_factory=list, description="Perf tests")

    # Configuration
    browsers: List[BrowserType] = Field(
        default_factory=lambda: [BrowserType.CHROMIUM],
        description="Browsers to test"
    )
    viewports: List[Viewport] = Field(default_factory=list, description="Viewports to test")

    # Page Objects
    page_objects: Dict[str, PageObjectModel] = Field(
        default_factory=dict,
        description="Page object models"
    )

    # Network
    network_mocks: List[NetworkMock] = Field(default_factory=list, description="Network mocks")

    # Parallel execution
    parallel: bool = Field(default=True, description="Run tests in parallel")
    workers: int = Field(default=4, description="Number of parallel workers")

    # Reporting
    generate_html_report: bool = Field(default=True, description="Generate HTML report")
    screenshot_on_failure: bool = Field(default=True, description="Screenshot on failure")
    video_on_failure: bool = Field(default=False, description="Record video on failure")

    # Metadata
    created_at: datetime = Field(default_factory=datetime.now)
    last_run: Optional[datetime] = Field(default=None)
```

### Implementation Tasks (ordered by dependencies)

```yaml
Task 1: INSTALL Playwright and dependencies
  - EXECUTE: npm install -D playwright @playwright/test
  - EXECUTE: npx playwright install chromium firefox webkit
  - CONFIGURE: Browser binary paths
  - PLACEMENT: Project dependencies

Task 2: CREATE src/browser/base.py
  - IMPLEMENT: BaseBrowserAutomation abstract class
  - FOLLOW pattern: Abstract base class with async methods
  - NAMING: BaseBrowserAutomation, launch_browser, execute_test methods
  - PLACEMENT: Browser subsystem base module

Task 3: CREATE src/models/browser_models.py
  - IMPLEMENT: Browser automation data models
  - FOLLOW pattern: Pydantic models with validation
  - NAMING: Models as specified in data models section
  - PLACEMENT: Models directory

Task 4: CREATE src/browser/playwright_integration.py
  - IMPLEMENT: PlaywrightManager for browser control
  - FOLLOW pattern: MCP server integration pattern
  - NAMING: PlaywrightManager, create_context, navigate, interact methods
  - DEPENDENCIES: Playwright library, MCP protocol
  - PLACEMENT: Browser subsystem

Task 5: CREATE src/browser/browser_manager.py
  - IMPLEMENT: BrowserContextManager for lifecycle management
  - FOLLOW pattern: Context manager pattern for resource cleanup
  - NAMING: BrowserContextManager, create_browser, close_browser methods
  - DEPENDENCIES: Playwright integration
  - PLACEMENT: Browser subsystem

Task 6: CREATE src/browser/page_analyzer.py
  - IMPLEMENT: PageAnalyzer for DOM analysis
  - FOLLOW pattern: DOM traversal and element extraction
  - NAMING: PageAnalyzer, analyze_page, extract_elements methods
  - DEPENDENCIES: Playwright page API
  - PLACEMENT: Browser subsystem

Task 7: CREATE src/browser/pom/pom_generator.py
  - IMPLEMENT: POMGenerator for Page Object Model creation
  - FOLLOW pattern: Template-based code generation
  - NAMING: POMGenerator, generate_pom, create_locators methods
  - DEPENDENCIES: Page analyzer, element selector
  - PLACEMENT: POM subsystem

Task 8: CREATE src/browser/journey_recorder.py
  - IMPLEMENT: JourneyRecorder for user action recording
  - FOLLOW pattern: Event listener and action tracking
  - NAMING: JourneyRecorder, start_recording, stop_recording, generate_test methods
  - DEPENDENCIES: Playwright events API
  - PLACEMENT: Browser subsystem

Task 9: CREATE src/browser/visual_tester.py
  - IMPLEMENT: VisualTester for screenshot comparison
  - FOLLOW pattern: Image diff algorithms
  - NAMING: VisualTester, capture_screenshot, compare_images, generate_diff methods
  - DEPENDENCIES: PIL/Pillow for image processing
  - PLACEMENT: Browser subsystem

Task 10: CREATE src/browser/accessibility_tester.py
  - IMPLEMENT: AccessibilityTester for WCAG validation
  - FOLLOW pattern: Axe-core integration
  - NAMING: AccessibilityTester, run_audit, check_wcag_compliance methods
  - DEPENDENCIES: axe-playwright or similar
  - PLACEMENT: Browser subsystem

Task 11: CREATE src/browser/performance_monitor.py
  - IMPLEMENT: PerformanceMonitor for metrics collection
  - FOLLOW pattern: Performance Observer API
  - NAMING: PerformanceMonitor, collect_metrics, analyze_cwv methods
  - DEPENDENCIES: Browser performance API
  - PLACEMENT: Browser subsystem

Task 12: CREATE src/browser/network_interceptor.py
  - IMPLEMENT: NetworkInterceptor for request mocking
  - FOLLOW pattern: Playwright route API
  - NAMING: NetworkInterceptor, mock_request, intercept_response methods
  - DEPENDENCIES: Playwright network API
  - PLACEMENT: Browser subsystem

Task 13: CREATE src/browser/pom/element_selector.py
  - IMPLEMENT: SmartSelector for robust element selection
  - FOLLOW pattern: Multiple selector strategies
  - NAMING: SmartSelector, generate_selector, validate_selector methods
  - DEPENDENCIES: DOM analysis
  - PLACEMENT: POM subsystem

Task 14: CREATE src/services/browser_service.py
  - IMPLEMENT: BrowserOrchestrator high-level service
  - FOLLOW pattern: Service facade pattern
  - NAMING: BrowserOrchestrator, run_e2e_tests, generate_pom methods
  - DEPENDENCIES: All browser components
  - PLACEMENT: Services layer

Task 15: CREATE src/tools/implementations/browser_tools.py
  - IMPLEMENT: Browser automation tools for agents
  - FOLLOW pattern: BaseTool inheritance from PRP-04
  - NAMING: RunE2ETestTool, CaptureScreenshotTool, CheckAccessibilityTool
  - DEPENDENCIES: Browser service
  - PLACEMENT: Tool implementations

Task 16: MODIFY src/agents/tester.py
  - INTEGRATE: Add E2E testing capabilities
  - FIND pattern: execute method, test generation
  - ADD: Browser test generation and execution
  - PRESERVE: Existing unit/integration test functionality

Task 17: CREATE src/browser/reporters/html_reporter.py
  - IMPLEMENT: HTMLReporter for test results
  - FOLLOW pattern: Template-based HTML generation
  - NAMING: HTMLReporter, generate_report, add_test_result methods
  - DEPENDENCIES: Jinja2 or similar
  - PLACEMENT: Reporters module

Task 18: CREATE src/tests/browser/test_playwright_integration.py
  - IMPLEMENT: Unit tests for Playwright integration
  - FOLLOW pattern: pytest-asyncio with mocks
  - COVERAGE: Browser launch, page navigation, element interaction
  - PLACEMENT: Browser test directory

Task 19: CREATE src/tests/browser/test_pom_generation.py
  - IMPLEMENT: Tests for POM generation
  - FOLLOW pattern: Mock DOM structures
  - COVERAGE: Element extraction, selector generation
  - PLACEMENT: Browser test directory

Task 20: CREATE src/tests/browser/test_visual_regression.py
  - IMPLEMENT: Tests for visual testing
  - FOLLOW pattern: Image comparison validation
  - COVERAGE: Screenshot capture, diff generation
  - PLACEMENT: Browser test directory
```

### Implementation Patterns & Key Details

```python
# Playwright MCP integration pattern
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
import asyncio
from typing import Optional, Dict, Any, List

class PlaywrightManager:
    """
    PATTERN: Manage Playwright browser instances
    CRITICAL: Proper cleanup to avoid resource leaks
    """

    def __init__(self):
        self.playwright = None
        self.browsers: Dict[str, Browser] = {}
        self.contexts: Dict[str, BrowserContext] = {}
        self.pages: Dict[str, Page] = {}

    async def initialize(self):
        """Initialize Playwright instance."""
        self.playwright = await async_playwright().start()

    async def launch_browser(
        self,
        browser_type: BrowserType = BrowserType.CHROMIUM,
        headless: bool = True,
        **options
    ) -> Browser:
        """
        Launch browser instance.
        PATTERN: Reuse browser instances when possible
        """
        if browser_type in self.browsers:
            return self.browsers[browser_type]

        browser_launcher = getattr(self.playwright, browser_type.value)
        browser = await browser_launcher.launch(
            headless=headless,
            **options
        )
        self.browsers[browser_type] = browser
        return browser

    async def create_context(
        self,
        browser: Browser,
        viewport: Optional[Viewport] = None,
        **options
    ) -> BrowserContext:
        """
        Create isolated browser context.
        CRITICAL: Each test needs its own context
        """
        context_options = {}

        if viewport:
            context_options["viewport"] = {
                "width": viewport.width,
                "height": viewport.height
            }
            context_options["device_scale_factor"] = viewport.device_scale_factor
            context_options["is_mobile"] = viewport.is_mobile
            context_options["has_touch"] = viewport.has_touch

        context = await browser.new_context(**context_options, **options)

        # Enable request interception if needed
        if options.get("intercept_requests"):
            await context.route("**/*", self._handle_route)

        return context

    async def cleanup(self):
        """
        Clean up all browser resources.
        CRITICAL: Must be called to prevent leaks
        """
        for page in self.pages.values():
            await page.close()

        for context in self.contexts.values():
            await context.close()

        for browser in self.browsers.values():
            await browser.close()

        if self.playwright:
            await self.playwright.stop()

# Page Object Model generation pattern
class POMGenerator:
    """
    PATTERN: Generate Page Object Models from DOM
    CRITICAL: Handle dynamic content and SPA frameworks
    """

    def __init__(self, page_analyzer: PageAnalyzer):
        self.page_analyzer = page_analyzer

    async def generate_pom(
        self,
        page: Page,
        framework: Optional[str] = None
    ) -> PageObjectModel:
        """
        Generate POM from page analysis.
        PATTERN: Extract semantic elements and actions
        """
        # Analyze page structure
        elements = await self.page_analyzer.analyze_page(page)

        # Detect framework if not specified
        if not framework:
            framework = await self._detect_framework(page)

        # Generate POM structure
        pom = PageObjectModel(
            id=f"pom_{page.url}",
            url=page.url,
            name=self._generate_page_name(page.url),
            framework=framework
        )

        # Add elements with smart selectors
        for element in elements:
            selector = await self._generate_selector(element, framework)
            pom.elements[element.name] = PageElement(
                name=element.name,
                selector=selector,
                element_type=element.type,
                attributes=element.attributes,
                aria_label=element.attributes.get("aria-label"),
                data_testid=element.attributes.get("data-testid")
            )

        # Generate actions based on elements
        pom.actions = self._generate_actions(pom.elements)

        # Extract components for framework-specific POMs
        if framework in ["react", "vue", "angular"]:
            pom.components = await self._extract_components(page, framework)

        return pom

    async def _generate_selector(
        self,
        element: Dict[str, Any],
        framework: str
    ) -> str:
        """
        Generate robust selector for element.
        PATTERN: Prefer stable attributes over position
        """
        # Priority order for selectors
        # 1. data-testid (most stable)
        if element.get("data-testid"):
            return f'[data-testid="{element["data-testid"]}"]'

        # 2. ID (if unique)
        if element.get("id"):
            return f'#{element["id"]}'

        # 3. Unique aria-label
        if element.get("aria-label"):
            return f'[aria-label="{element["aria-label"]}"]'

        # 4. Framework-specific attributes
        if framework == "react" and element.get("key"):
            return f'[key="{element["key"]}"]'

        # 5. Combination of tag and attributes
        tag = element.get("tag", "*")
        attrs = []
        for attr in ["name", "type", "role"]:
            if element.get(attr):
                attrs.append(f'[{attr}="{element[attr]}"]')

        return f'{tag}{"".join(attrs)}'

# User journey recording pattern
class JourneyRecorder:
    """
    PATTERN: Record and replay user interactions
    CRITICAL: Capture timing and wait conditions
    """

    def __init__(self):
        self.recording = False
        self.journey: Optional[UserJourney] = None
        self.steps: List[JourneyStep] = []

    async def start_recording(
        self,
        page: Page,
        journey_name: str
    ) -> None:
        """
        Start recording user interactions.
        PATTERN: Attach event listeners to page
        """
        self.recording = True
        self.journey = UserJourney(
            id=f"journey_{journey_name}",
            name=journey_name,
            start_url=page.url,
            steps=[]
        )

        # Attach event listeners
        await page.expose_function("recordAction", self._record_action)

        # Inject recording script
        await page.add_init_script("""
            window.addEventListener('click', (e) => {
                window.recordAction({
                    action: 'click',
                    target: e.target,
                    timestamp: Date.now()
                });
            });

            window.addEventListener('input', (e) => {
                window.recordAction({
                    action: 'fill',
                    target: e.target,
                    value: e.target.value,
                    timestamp: Date.now()
                });
            });
        """)

    async def _record_action(self, action_data: Dict[str, Any]):
        """Record user action as journey step."""
        if not self.recording:
            return

        step = JourneyStep(
            step_number=len(self.steps) + 1,
            action=action_data["action"],
            target=self._generate_selector(action_data["target"]),
            value=action_data.get("value"),
            wait_for="networkidle" if action_data["action"] == "navigate" else None
        )

        self.steps.append(step)

    async def generate_test(self) -> str:
        """
        Generate test code from recorded journey.
        PATTERN: Playwright test syntax
        """
        test_code = f"""
import {{ test, expect }} from '@playwright/test';

test('{self.journey.name}', async ({{ page }}) => {{
    // Navigate to starting page
    await page.goto('{self.journey.start_url}');

    // Execute journey steps
"""

        for step in self.steps:
            if step.action == "click":
                test_code += f"    await page.click('{step.target}');\n"
            elif step.action == "fill":
                test_code += f"    await page.fill('{step.target}', '{step.value}');\n"
            elif step.action == "navigate":
                test_code += f"    await page.goto('{step.value}');\n"

            if step.wait_for:
                test_code += f"    await page.waitForLoadState('{step.wait_for}');\n"

            if step.validate:
                test_code += f"    await expect(page.locator('{step.target}')).{step.validate};\n"

        test_code += "});"
        return test_code

# Visual regression testing pattern
import numpy as np
from PIL import Image
import imagehash

class VisualTester:
    """
    PATTERN: Screenshot comparison with tolerance
    CRITICAL: Handle anti-aliasing and dynamic content
    """

    def __init__(self, threshold: float = 0.01):
        self.threshold = threshold  # 1% difference allowed

    async def capture_screenshot(
        self,
        page: Page,
        name: str,
        full_page: bool = False
    ) -> str:
        """Capture page screenshot."""
        screenshot_path = f"screenshots/{name}.png"
        await page.screenshot(
            path=screenshot_path,
            full_page=full_page
        )
        return screenshot_path

    def compare_images(
        self,
        baseline_path: str,
        actual_path: str,
        ignore_regions: List[Dict[str, int]] = None
    ) -> VisualTestResult:
        """
        Compare two images for visual differences.
        PATTERN: Pixel-by-pixel with tolerance
        """
        baseline_img = Image.open(baseline_path)
        actual_img = Image.open(actual_path)

        # Ensure same dimensions
        if baseline_img.size != actual_img.size:
            return VisualTestResult(
                test_id="size_mismatch",
                page_url="",
                baseline_path=baseline_path,
                actual_path=actual_path,
                match_percentage=0.0,
                pixel_difference=-1,
                is_match=False
            )

        # Apply ignore regions if specified
        if ignore_regions:
            baseline_img = self._apply_ignore_regions(baseline_img, ignore_regions)
            actual_img = self._apply_ignore_regions(actual_img, ignore_regions)

        # Convert to numpy arrays
        baseline_array = np.array(baseline_img)
        actual_array = np.array(actual_img)

        # Calculate difference
        diff = np.abs(baseline_array.astype(float) - actual_array.astype(float))

        # Count different pixels (with tolerance)
        diff_mask = diff.max(axis=2) > (255 * self.threshold)
        pixel_difference = diff_mask.sum()

        total_pixels = baseline_img.size[0] * baseline_img.size[1]
        match_percentage = 1.0 - (pixel_difference / total_pixels)

        # Generate diff image if there are differences
        diff_path = None
        if pixel_difference > 0:
            diff_path = self._generate_diff_image(
                baseline_array,
                actual_array,
                diff_mask
            )

        return VisualTestResult(
            test_id=f"visual_{name}",
            page_url="",
            baseline_path=baseline_path,
            actual_path=actual_path,
            diff_path=diff_path,
            match_percentage=match_percentage,
            pixel_difference=pixel_difference,
            is_match=match_percentage >= (1.0 - self.threshold),
            threshold=self.threshold
        )

# Accessibility testing pattern
class AccessibilityTester:
    """
    PATTERN: WCAG compliance validation
    CRITICAL: Some criteria require manual review
    """

    async def run_audit(
        self,
        page: Page,
        wcag_level: str = "AA"
    ) -> List[AccessibilityIssue]:
        """
        Run accessibility audit on page.
        PATTERN: Axe-core integration
        """
        # Inject axe-core
        await page.add_script_tag(
            url="https://cdnjs.cloudflare.com/ajax/libs/axe-core/4.7.2/axe.min.js"
        )

        # Run audit
        results = await page.evaluate("""
            () => {
                return new Promise((resolve) => {
                    axe.run(document, {
                        runOnly: {
                            type: 'tag',
                            values: ['wcag2a', 'wcag2aa', 'best-practice']
                        }
                    }).then(results => resolve(results));
                });
            }
        """)

        # Parse results into issues
        issues = []
        for violation in results.get("violations", []):
            for node in violation.get("nodes", []):
                issue = AccessibilityIssue(
                    id=f"{violation['id']}_{node['target'][0]}",
                    impact=violation["impact"],
                    rule_id=violation["id"],
                    description=violation["description"],
                    help_text=violation["help"],
                    selector=node["target"][0],
                    html=node["html"],
                    wcag_criteria=violation.get("tags", []),
                    wcag_level=wcag_level,
                    fix_suggestion=violation.get("helpUrl")
                )
                issues.append(issue)

        return issues

# Performance monitoring pattern
class PerformanceMonitor:
    """
    PATTERN: Collect Core Web Vitals and metrics
    CRITICAL: Metrics vary by hardware/network
    """

    async def collect_metrics(self, page: Page) -> PerformanceMetrics:
        """
        Collect performance metrics from page.
        PATTERN: Performance Observer API
        """
        # Inject performance collection script
        metrics = await page.evaluate("""
            () => {
                return new Promise((resolve) => {
                    // Collect navigation timing
                    const navTiming = performance.getEntriesByType('navigation')[0];

                    // Collect paint timing
                    const paintTiming = performance.getEntriesByType('paint');

                    // Collect LCP
                    new PerformanceObserver((list) => {
                        const entries = list.getEntries();
                        const lastEntry = entries[entries.length - 1];

                        resolve({
                            lcp: lastEntry.renderTime || lastEntry.loadTime,
                            fcp: paintTiming.find(p => p.name === 'first-contentful-paint')?.startTime,
                            ttfb: navTiming.responseStart - navTiming.fetchStart,
                            dom_content_loaded: navTiming.domContentLoadedEventEnd - navTiming.fetchStart,
                            load_complete: navTiming.loadEventEnd - navTiming.fetchStart,
                            total_requests: performance.getEntriesByType('resource').length,
                            js_heap_size_mb: performance.memory?.usedJSHeapSize / 1048576
                        });
                    }).observe({ entryTypes: ['largest-contentful-paint'] });
                });
            }
        """)

        # Calculate CLS
        cls = await page.evaluate("""
            () => {
                let cls = 0;
                new PerformanceObserver((list) => {
                    for (const entry of list.getEntries()) {
                        if (!entry.hadRecentInput) {
                            cls += entry.value;
                        }
                    }
                }).observe({ entryTypes: ['layout-shift'] });
                return cls;
            }
        """)

        return PerformanceMetrics(
            url=page.url,
            lcp=metrics["lcp"],
            fid=0,  # FID requires user interaction
            cls=cls,
            ttfb=metrics["ttfb"],
            fcp=metrics["fcp"],
            tti=metrics.get("tti", 0),
            speed_index=0,  # Requires additional calculation
            dom_content_loaded=metrics["dom_content_loaded"],
            load_complete=metrics["load_complete"],
            total_requests=metrics["total_requests"],
            total_size_kb=0,  # Requires network analysis
            js_heap_size_mb=metrics.get("js_heap_size_mb", 0),
            passes_cwv=self._check_cwv_thresholds(metrics),
            browser=BrowserType.CHROMIUM
        )

    def _check_cwv_thresholds(self, metrics: Dict[str, Any]) -> bool:
        """Check if metrics pass Core Web Vitals thresholds."""
        return (
            metrics.get("lcp", float('inf')) < 2500 and  # LCP < 2.5s
            metrics.get("fid", float('inf')) < 100 and   # FID < 100ms
            metrics.get("cls", float('inf')) < 0.1       # CLS < 0.1
        )
```

### Integration Points

```yaml
PLAYWRIGHT:
  - installation: "npm install -D playwright @playwright/test"
  - browsers: "npx playwright install"
  - configuration: "playwright.config.ts"

MCP_SERVER:
  - integration: "Playwright MCP server for browser control"
  - protocol: "JSON-RPC over stdio"
  - pattern: "Tool registration with browser actions"

TESTING_SERVICE:
  - integration: "Extend TestingOrchestrator from PRP-09"
  - pattern: "Add E2E test generation"
  - enhancement: "Browser test execution"

TESTER_AGENT:
  - integration: "Add browser testing capabilities"
  - pattern: "Execute E2E tests via tools"
  - coordination: "With validator for results"

AST_PARSER:
  - integration: "Analyze frontend components"
  - pattern: "Extract component structure for POM"
  - usage: "React/Vue/Angular component analysis"

TOOL_SERVICE:
  - integration: "Register browser automation tools"
  - pattern: "BaseTool implementation"
  - tools: "RunE2ETest, CaptureScreenshot, CheckAccessibility"

CONFIG:
  - add to: .env
  - variables: |
      # Browser Configuration
      BROWSER_HEADLESS=true
      BROWSER_TIMEOUT_MS=30000
      BROWSER_SLOW_MO=0
      BROWSER_DEVTOOLS=false

      # Test Configuration
      E2E_PARALLEL_WORKERS=4
      E2E_RETRY_COUNT=2
      E2E_DEFAULT_VIEWPORT_WIDTH=1280
      E2E_DEFAULT_VIEWPORT_HEIGHT=720

      # Visual Testing
      VISUAL_THRESHOLD=0.01
      VISUAL_IGNORE_ANTIALIASING=true
      VISUAL_IGNORE_COLORS=false

      # Accessibility
      WCAG_LEVEL=AA
      A11Y_INCLUDE_WARNINGS=false

      # Performance
      PERF_THROTTLE_CPU=4
      PERF_THROTTLE_NETWORK=fast3g

      # Screenshots
      SCREENSHOT_PATH=./screenshots
      SCREENSHOT_FULL_PAGE=false

      # Network
      NETWORK_MOCK_ENABLED=true
      NETWORK_CACHE_DISABLED=false

DEPENDENCIES:
  - playwright: "Browser automation library"
  - @playwright/test: "Test runner"
  - axe-playwright: "Accessibility testing"
  - pixelmatch: "Visual regression"
  - playwright-video: "Video recording"
```

## Validation Loop

### Level 1: Syntax & Style (Immediate Feedback)

```bash
# After creating each file
ruff check src/browser/ --fix
mypy src/browser/ --strict
ruff format src/browser/

# Verify imports
python -c "from src.browser import PlaywrightManager; print('Browser imports OK')"
python -c "from src.services.browser_service import BrowserOrchestrator; print('Service imports OK')"

# Verify Playwright installation
npx playwright --version
npx playwright show-report

# Expected: Zero errors, all imports resolve, Playwright installed
```

### Level 2: Unit Tests (Component Validation)

```bash
# Test Playwright integration
pytest src/tests/browser/test_playwright_integration.py -v --cov=src/browser/playwright_integration

# Test POM generation
pytest src/tests/browser/test_pom_generation.py -v --cov=src/browser/pom

# Test visual regression
pytest src/tests/browser/test_visual_regression.py -v --cov=src/browser/visual_tester

# Test accessibility
pytest src/tests/browser/test_accessibility.py -v --cov=src/browser/accessibility_tester

# Full browser test suite
pytest src/tests/browser/ -v --cov=src/browser --cov-report=term-missing

# Expected: 85%+ coverage, all tests pass
```

### Level 3: Integration Testing (System Validation)

```bash
# Test browser launch and control
python scripts/test_browser_launch.py \
  --browsers chromium firefox webkit \
  --verify-launch \
  --check-contexts
# Expected: All browsers launch successfully

# Test POM generation on real site
python scripts/test_pom_generation_real.py \
  --url https://example.com \
  --analyze-dom \
  --generate-pom \
  --verify-selectors
# Expected: Valid POM with working selectors

# Test user journey recording
python scripts/test_journey_recording.py \
  --url https://demo.playwright.dev/todomvc \
  --record-actions \
  --generate-test \
  --replay-journey
# Expected: Journey recorded and replayed successfully

# Test visual regression setup
python scripts/test_visual_setup.py \
  --capture-baselines \
  --make-changes \
  --run-comparison \
  --verify-diffs
# Expected: Visual differences detected accurately

# Test accessibility validation
python scripts/test_accessibility_validation.py \
  --url https://www.w3.org/WAI/demos/bad/ \
  --check-wcag-aa \
  --generate-report
# Expected: Accessibility issues detected

# Test performance monitoring
python scripts/test_performance_monitoring.py \
  --url https://web.dev \
  --collect-metrics \
  --check-cwv \
  --verify-thresholds
# Expected: Core Web Vitals collected

# Test network mocking
python scripts/test_network_mocking.py \
  --mock-api-responses \
  --verify-mocks \
  --check-fallbacks
# Expected: Network requests intercepted and mocked
```

### Level 4: Creative & Domain-Specific Validation

```bash
# Cross-browser compatibility test
python scripts/test_cross_browser.py \
  --test-suite ./e2e-tests/ \
  --browsers all \
  --compare-results \
  --identify-differences
# Expected: Consistent behavior across browsers

# Mobile responsiveness testing
python scripts/test_mobile_responsiveness.py \
  --devices "iPhone 12,Pixel 5,iPad" \
  --test-viewports \
  --check-layouts \
  --verify-touch
# Expected: Responsive design validated

# Complex user journey testing
python scripts/test_complex_journey.py \
  --scenario "e-commerce-checkout" \
  --multi-step \
  --validate-state \
  --check-persistence
# Expected: Multi-step journey completes successfully

# Visual regression with dynamic content
python scripts/test_dynamic_visual.py \
  --handle-animations \
  --ignore-dynamic-regions \
  --smart-wait \
  --verify-stability
# Expected: Stable visual testing despite dynamic content

# Performance budget validation
python scripts/test_performance_budget.py \
  --budget-config ./perf-budget.json \
  --run-tests \
  --check-violations \
  --generate-report
# Expected: Performance within budget constraints

# Accessibility compliance audit
python scripts/test_full_accessibility_audit.py \
  --wcag-level AAA \
  --include-best-practices \
  --manual-checks \
  --generate-vpat
# Expected: Comprehensive accessibility report

# Parallel execution stress test
python scripts/test_parallel_execution.py \
  --workers 10 \
  --tests 100 \
  --measure-time \
  --check-isolation
# Expected: 60%+ speed improvement, no interference

# POM maintenance test
python scripts/test_pom_maintenance.py \
  --change-ui \
  --update-pom \
  --verify-tests \
  --measure-effort
# Expected: POM updates handle UI changes

# Integration with CI/CD
python scripts/test_cicd_integration.py \
  --github-actions \
  --docker-setup \
  --headless-mode \
  --artifact-collection
# Expected: Seamless CI/CD integration

# Agent integration test
python scripts/test_tester_agent_e2e.py \
  --task "Test checkout flow" \
  --generate-e2e-tests \
  --execute-tests \
  --verify-coverage
# Expected: Tester agent generates and runs E2E tests
```

## Final Validation Checklist

### Technical Validation

- [ ] All 4 validation levels completed successfully
- [ ] Browser tests achieve 85%+ coverage: `pytest src/tests/browser/ --cov=src/browser`
- [ ] No linting errors: `ruff check src/browser/`
- [ ] No type errors: `mypy src/browser/ --strict`
- [ ] Playwright browsers installed and working

### Feature Validation

- [ ] 3+ browsers supported (Chromium, Firefox, WebKit)
- [ ] POM generation from DOM analysis working
- [ ] User journey recording and playback functional
- [ ] Visual regression testing operational
- [ ] Accessibility testing with WCAG reports
- [ ] Performance metrics collection working
- [ ] Network mocking capabilities verified
- [ ] Parallel execution reducing test time by 60%+

### Code Quality Validation

- [ ] Follows existing agent and service patterns
- [ ] All operations async-compatible
- [ ] Proper browser resource management
- [ ] Robust element selectors generated
- [ ] Test isolation maintained
- [ ] Error handling for flaky tests

### Documentation & Deployment

- [ ] Environment variables documented
- [ ] Browser setup instructions provided
- [ ] POM templates documented
- [ ] Visual testing baseline management explained
- [ ] CI/CD integration guide created
- [ ] Performance budget configuration documented

---

## Anti-Patterns to Avoid

- ❌ Don't leave browser contexts open (resource leaks)
- ❌ Don't use fragile selectors (position-based)
- ❌ Don't ignore timing issues (explicit waits needed)
- ❌ Don't skip viewport testing (mobile responsiveness)
- ❌ Don't hardcode test data in POMs
- ❌ Don't run tests without proper isolation
- ❌ Don't ignore accessibility false positives
- ❌ Don't compare screenshots pixel-perfect without tolerance
- ❌ Don't skip network mocking for external dependencies
- ❌ Don't forget to validate across different browsers
- ❌ Don't neglect performance impact of parallel execution
- ❌ Don't generate tests without assertions
